{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2f39bfa-8bba-4883-a1c6-5bf8f5d61476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re\n",
    "from pinecone import Pinecone , ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.schema import Document\n",
    "from gtts import gTTS\n",
    "import io\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc74759c-2f7e-46ef-a10b-c3ec650fd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "filenames = [\n",
    "    \"021.txt\",\n",
    "    \"2477the-hard-thing-about-hard-things.txt\",\n",
    "    \"Atomic habits ( PDFDrive ).txt\",\n",
    "    \"angela-duckworth-grit.txt\",\n",
    "    \"Hooked-How-to-Build-Habit-Forming-Products-_Nir-Eyal_.txt\",\n",
    "    \"Measure-What-Matters-John-Doerr.txt\",\n",
    "    \"Rich-Dad-Poor-Dad-eBook.txt\",\n",
    "    \"Super Founders PDF.txt\",\n",
    "    \"The Lean Startup - Erick Ries.txt\",\n",
    "    \"The-100-Startup-Chris-Guillebeau.txt\",\n",
    "    \"The-100-Startup-Chris-Guillebeau.txt\",\n",
    "    \"Venture-deals.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e3d89f1-0241-416a-a8cc-d39291001017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,:;!?\\'\"-]', '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2246b2a5-35ce-443b-bd74-a04197438957",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = \"\"\n",
    "for file in filenames:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "        combined_data += cleaned_text + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b9c263-6e85-4797-a583-27d47ccbc56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Copyright 2014 by Peter Thiel All rights reserved. Published in the United States by Crown Business, an imprint of the Crown Publishing Group, a division of Random House LLC, a Penguin Random House Company, New York. www.crownpublishing.com CROWN BUSINESS is a trademark and CROWN and the Rising Sun colophon are registered trademarks of Random House LLC. Crown Business books are available at special discounts for bulk purchases for sales promotions or corporate use. Special editions, including pe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84677d6-3845-431b-90cf-afb9ed506ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [combined_data[i:i+1000] for i in range(0 , len(combined_data) , 1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c3842eb-0f47-4507-9e70-0628431ddede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saving, at least they could expect to have money to spend later. And if American companies were investing, they could expect to reap the rewards of new wealth in the future. But U.S. households are saving almost nothing. And U.S. companies are letting cash pile up on their balance sheets without investing in new projects because they dont have any concrete plans for the future. The other three views of the future can work. Definite optimism works when you build the future you envision. Definite pessimism works by building what can be copied without expecting anything ne w. Indefinite pessimism works because its self-fulfilling: if youre a slacker with low expectations, theyll probably be met. But indefinite optimism seems inherently unsustainable: how can the future get better if no one plans for it? Actually, most everybody in the modern world has already heard an answer to this question: progress without planning is what we call evolution. Darwin himself wrote that life tends to prog'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcb738b6-225c-41ac-a157-25846881c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pine_cone = os.getenv('pine_cone')\n",
    "groq = os.getenv('groq')\n",
    "hugging_face = os.getenv(\"hugging_face\")\n",
    "eleven_lab = os.getenv(\"eleven_lab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ceabcd-7f15-411a-acb5-9ccc24445653",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe92357d-405f-4883-bff6-77119cff1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f9c6d3-9ad5-4c3b-98bc-05a475614f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankus\\AppData\\Local\\Temp\\ipykernel_1660\\603423264.py:1: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\",\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "                                    model_kwargs={\"token\" : hugging_face})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db995a3-947d-4f3c-bae9-93dc2900603e",
   "metadata": {},
   "source": [
    "## pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f45d425-6b55-4525-9da0-b5c77d21fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key = pine_cone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba03a55d-8e66-4848-ad2f-08a96ed6f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"teacher\"\n",
    "if index not in pc.list_indexes().names():\n",
    "    pc.create_index(name=index,\n",
    "    dimension=768,\n",
    "    metric= 'cosine',\n",
    "    spec = ServerlessSpec(region = \"us-east-1\" , cloud = \"AWS\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda16ef1-f0bf-4555-82ee-0976c6a0cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pc.Index(index ,host=os.getenv('host_bg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3ca4c8-fa3c-41e2-b05e-7b04bb4705bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content = chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a9a4bb8-ebd0-49d9-9645-f4923c4265ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Title. HD62.5.T525 2014 685.11dc23 2014006653 Hardcover ISBN: 978-0-8041-3929-8 eBook ISBN: 978-0-8041-3930-4 Book design by Ralph Fowler  rlfdesign Graphics by Rodrigo Corral Design Illustrations by Matt Buck Cover design by Michael Nagin Additional credits appear on this page, which constitutes a continuation of this copyright page. v3.1 Contents Preface: Zero to One 1 The Challenge of the Future 2 Party Like Its 1999 3 All Happy Companies Are Different 4 The Ideology of Competition 5 Last Mover Advantage 6 You Are Not a Lottery Ticket 7 Follow the Money 8 Secrets 9 Foundations 10 The Mechanics of Mafia 11 If You Build It, Will They Come? 12 Man and Machine 13 Seeing Green 14 The Founders Paradox Conclusion: Stagnation or Singularity? Acknowledgments Illustration Credits Index About the Authors Preface ZERO TO ONE EVERY MOMENT IN BUSINESS happens only once. The next Bill Gates will not build an operating system. The next Larry Page or Sergey Brin wont make a search engine. And the ne')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb17b426-4e60-4a0a-88a6-e10eea413cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = PineconeVectorStore(index=index,\n",
    "    embedding=embedding,\n",
    "    text_key=\"page_content\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f76ca63-42c5-4c3e-ae58-9d087c0a55bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 154/154 [07:21<00:00,  2.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "batch_size = 32\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    try:\n",
    "        vector.add_documents(batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch {i}-{i+batch_size}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aba14ff-cc9e-452e-acd2-fabde9237b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import create_history_aware_retriever  , create_retrieval_chain\n",
    "from langchain_core.prompts import MessagesPlaceholder , ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53ab8a94-d656-40b0-95ec-9fbbf64a76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(api_key = groq , temperature=0.5 , model =\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbb7dc05-4089-446d-a4ae-64f14da2b304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 37, 'total_tokens': 47, 'completion_time': 0.099461733, 'prompt_time': 0.001846167, 'queue_time': 0.055530711999999996, 'total_time': 0.1013079}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_6507bcfb6f', 'finish_reason': 'stop', 'logprobs': None}, id='run--45068fc3-b0ba-45cb-b7c5-d1292201af4c-0', usage_metadata={'input_tokens': 37, 'output_tokens': 10, 'total_tokens': 47})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"hii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3889240a-a04d-49ca-8589-ab1a923e5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver_prompt = (\"Given a chat history and the latest user question which might reference context in the chat history,\"\n",
    "    \"formulate a standalone question which can be understood without the chat history.\"\n",
    "    \"Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81ae810f-ca01-4a1b-8e66-330b59986e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrive = vector.as_retriever(search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e57f58a0-2dea-4399-8baa-5518ee2cedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_qa = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",retriver_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\",'{input}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a9b999a-d431-4c91-bbe6-b859b67edbaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001A199DA65F0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Given a chat history and the latest user question which might reference context in the chat history,formulate a standalone question which can be understood without the chat history.Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextual_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bad7e8d6-4e32-4239-9b2e-1c3cf345ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = create_history_aware_retriever(model , retrive , contextual_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e5ad880-3d93-45eb-a685-bfd1b8dd4162",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOT = \"\"\"You are an AI Startup Coach, specifically designed to guide entrepreneurs during tough startup situations using wisdom from top entrepreneurship books.\n",
    "\n",
    "Whenever a user shares a startup problem (e.g., failure, funding stress, team issues, growth confusion, burnout), follow this structure strictly:\n",
    "\n",
    "1. Carefully understand the user’s problem.\n",
    "2. Identify if it is truly **startup/business-related**.\n",
    "   - If NOT, politely decline:  \n",
    "     \"I'm designed to help with startup challenges. Please ask a question related to your startup journey.\"\n",
    "     if nessesory then refer the book and author name otherwise don't take book name and author name only genrate relevent query\n",
    "\n",
    "Make sure:\n",
    "- You **do NOT hallucinate** fake answers.\n",
    "- If no book supports the answer, say:  \n",
    "  “This isn’t directly covered in my book knowledge, but based on similar principles, here’s a possible direction…”\n",
    "- Keep answers short, focused, and strategic. No storytelling unless highly relevant.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {input}\n",
    "\n",
    "YOUR ANSWER:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fdb23225-e85b-41e1-8625-4804dc688c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_bot = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",BOT),\n",
    "        MessagesPlaceholder(variable_name = \"chat_history\"),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "850a22c9-279b-4031-a963-4d4b896479cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(model,qa_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d56588ad-f2d7-458f-83ae-1ad82ed1d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_retrieval_chain(history , question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c5608069-1442-40a0-b7d3-820d82cef328",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "09f54c8e-af6d-4144-be6b-e12ef0e155f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd5437a0-756d-43f7-8f12-be2731b3c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_history(session_id: str)-> BaseChatMessageHistory:\n",
    "  if session_id not in store:\n",
    "    store[session_id]= ChatMessageHistory()\n",
    "  return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7be4c692-89b9-474d-baa5-409222f60acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "chat_with_memory = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9c611813-10d7-48a2-a53e-590fab5e7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_text(text: str,speed: float = 1.5):\n",
    "    # Convert text to speech and write to memory\n",
    "    tts = gTTS(text)\n",
    "    fp = io.BytesIO()\n",
    "    tts.write_to_fp(fp)\n",
    "    fp.seek(0)\n",
    "    \n",
    "    # Initialize pygame mixer (only once)\n",
    "    if not pygame.mixer.get_init():\n",
    "        pygame.mixer.init()\n",
    "\n",
    "    # Load and play audio\n",
    "    pygame.mixer.music.load(fp, 'mp3')\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Wait until playback is finished\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c77f90e4-7cf1-46e5-8560-19ff8a89bbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hii mai self ankush\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm designed to help with startup challenges. Please ask a question related to your startup journey.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  my self jai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: I'm designed to help with startup challenges. Please ask a question related to your startup journey.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is mvp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: A Minimum Viable Product (MVP) is a product with just enough features to satisfy early customers and provide feedback for future development, as described in \"The Lean Startup\" by Eric Ries. It's the fastest way to get through the Build-Measure-Learn feedback loop with minimal effort.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  actually my startup is for gamming how i grow this fast as soon as possible\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: To grow your gaming startup quickly, focus on understanding your target audience and delivering a high-quality user experience. As mentioned in \"The Lean Startup\" by Eric Ries, build a Minimum Viable Product (MVP) to test your assumptions and gather feedback. Additionally, consider the advice from \"Play Bigger\" by Christopher Lochhead, which emphasizes the importance of creating a unique category and owning it. Identify your niche in the gaming industry and develop a go-to-market strategy that resonates with your target audience.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  how i target audience\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: To target your audience for your gaming startup, consider the advice from \"Traction\" by Gabriel Weinberg and Justin Mares. Identify your ideal customer persona by examining demographics, preferences, and behaviors of gamers. Ask yourself: What type of games do they play? What platforms do they use? What are their pain points? Create a survey or gather feedback from potential customers to validate your assumptions. You can also analyze online communities, social media, and gaming forums to understand your target audience's interests and needs. As suggested in \"Blue Ocean Strategy\" by W. Chan Kim and Renée Mauborgne, focus on a specific niche to differentiate yourself from competitors and attract a dedicated audience.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_input\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai_env\\lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\genai_env\\lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "        break\n",
    "\n",
    "    response = chat_with_memory.invoke(\n",
    "        {\"input\": user_input},\n",
    "        config={\n",
    "            \"configurable\": {\"session_id\": \"143\"}\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(\"AI:\", response[\"answer\"])\n",
    "    speak_text(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eeece7-5873-4079-95b1-55b9c720e01c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_env",
   "language": "python",
   "name": "genai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
